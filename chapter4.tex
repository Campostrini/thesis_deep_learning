\chapter{Discussion and Conclusions}\label{chap4}
\thispagestyle{plain}
 In this chapter I will discuss the results by answering the research questions followed by a part on the decision taken for the methodology. Finally, I will compare the results with similar studies and state some possible developments.
\section{Results}

The answers to the research questions of \cref{sec:researchquestions} follow:
\begin{enumerate}
    \item \textbf{Does the employed neural network outperform persistence?} For most of the metrics and classes the neural network outperforms persistence. This is also what the macro-averages indicate. The macro averaged HSS for the neural network is 0.436 compared to 0.392 for persistence. The macro-averaged CSI is 0.397 for the neural network compared to 0.370 for persistence.
    \item \textbf{How does the model's performance depend on the rainfall intensity?} The performance of the model, in absolute terms decreases with rainfall intensity but gets generally better compared to persistence for increasing rainfall intensity.
    \item \textbf{The model of \citet{Agrawal2019MachineImages} after which this project is inspired outperformed persistence for all precipitation classes. Can this result be confirmed?}
    The model of this thesis was also found to generally outperform persistence albeit not to the same degree as the original paper. However, the ability of the model to abstract features as indicated by \cref{fig:examplescene1,fig:examplescene2} suggests that with further hyparameter tuning a similar performance could be achieved.
    \end{enumerate}



% ==== Section 1 ============================================================
\section{Discussion of the Methodology}
In this section I will discuss the decisions taken as regards to the methodology.

\subsection{Choice of the Network Architecture}
The network architecture of this project is chosen to resemble the ubiquitous U-Net architecture introduced by \citet{Ronneberger2015U-Net:Segmentation} that was found to perform well on segmentation tasks. As for the particular details of the various layers composing the network, I followed \citet{Agrawal2019MachineImages}. I treated the model depth as a hyperparameter given that \citet{Ayzel2020RainNetNowcasting}, trying to solve a similar problem, adopts a depth of 4 (instead of 7 as in \citet{Agrawal2019MachineImages}. An advantage of lower depth is decreased memory footprint and computational requirements. However, not enough parameters can affect the model performance. Unfortunately, \cite{Agrawal2019MachineImages} are not exhaustive in their description of the architecture. The lacking information was compensated by adopting solutions available in the general deep learning literature. The architecture of short-range skip connections when downsampling blocks are applied and the number of input feature maps to the blocks are different from the number of output feature maps \citep{Drozdzal2016TheSegmentation}. 

\subsection{Changes in the RADOLAN Routine}
The RADOLAN calibration routine described in \cref{sec:radolan} is subject to constant changes that are brought into the operational procedure. \citet{Weigl2021RADOLANRoutine} keeps track of these changes. Some are as as simple as additional ground rain gauges or the dismissal and addition of radar stations. Other changes affect the algorithm of the routine itself, such as better corrections for the shadowing of radar signals by the orography. The DWD has until now reprocessed the RADOLAN database twice, these data are available and could be used to improve the model performance.

\subsection{Choice of the Training, Validation and Test Sets}
It is common practice in the artificial intelligence literature to subdivide the dataset into three subsets, the training set used for optimizing the model, the validation set to compare the different hyperparameter choices and the test set to produce the final data later reported as the actual performance of the model. The proportions 80:10:10 are common choice for deep learning tasks. The choice of chronological order was taken partly also considering the changes in the RADOLAN calibration procedure, hence allowing the final performance measure to be closer to the performance if the model were to be applied to very recent data. A significant drawback of this approach is that it relies on the assumption that the time-series is stationary, i.e. no changes occurred to the underlying processes, in this case the climate. An alternative way to asses model performance avoiding the use of the computationally expensive cross validation, would be to choose the training set by randomly selecting members of the dataset.

\subsection{Area and Time-Resolution Selection}
Orography is known to introduce error in the final product derived from radar reflectivity measurements. For simplicity and to remain within the investigative and non-operational nature of the project I decided to select a limited area in a flat region of Germany completely within the RADOLAN RW product's domain. As it pertains to the choice of 1hr cumulative precipitation data, this was driven by the lack of literature applying deep learning weather prediction at those timescales. \citet{Agrawal2019MachineImages} uses 7 images at 10min distance from each other taken from the multi radar multi sensor (MRMS) dataset and predicts the MRMS image at 1h after the last image of the input. \citet{Ayzel2020RainNetNowcasting} in contrast uses 4 images at 5min interval from the RADOLAN RY product to predict a similar image 5min after the last one of the input series.

\subsection{Alternative Comparisons}
The model in this thesis was compared only with the so-called persistence prediction that works by assuming that the forecast is equal to the data at the last available time before the forecast. Despite its simplicity, persistence is not a trivial prediction to outperform and serves as first intuitive benchmark to understand the performance of a prediction method. \citet{Agrawal2019MachineImages} after which this thesis is partly inspired, compared their model with both persistence and the HRRR model. \citet{Ayzel2020RainNetNowcasting} too compared their model against persistence. They also compared it with the results of the \verb|rainymotion| \citep{Ayzel2019OpticalV0.1} optical flow library albeit at a 5min time resolution. The \verb|rainymotion| library was shown to outperform the RADVOR \citep{DeutscherWetterdienst2022RADVORRadar-Niederschlagsvorhersage} operational nowcast model of the DWD \citep{Ayzel2019OpticalV0.1}. Given the similarity of the dataset between this study and \citet{Ayzel2020RainNetNowcasting} further comparisons could include the accumulated hourly product produced by the RADVOR nowcast. The application of an optical flow algorithm such as \verb|rainymotion| to the 1h accumulated rainfall data needs to be investigated.

\subsection{Hyperparameters Optimization}
For the optimization of the hyperparameters of the model, I applied the manual search using knowledge guided by the literature as regards to the most plausible hyperparameter choice. As discussed in \cref{sec:hyperparametersearch} different methods to find the best combination of hyperparameters exist. It is likely that the application of an algorithm such as random search \cite{Bergstra2012RandomOptimization} could improve the results. However, considering the investigative purpose of this project, the increased amount of time and resources needed to train the model with multiple hyperparameters combinations would be beyond the current scope.

\subsection{Class Imbalances}
An imbalanced dataset is a common problem in classification tasks. The relevance is even more pronounced if the less frequent class is also of higher importance. For this project, as shown in \cref{tab:classdistributions} the high precipitation classes are vastly underrepresented in the dataset compared to the dominant no-rainfall class. The solution I employed, following a common practice in the deep learning community is to assign weights proportional to the inverse of the frequency of the classes for the purpose of computing the loss. However, this choice seems to lead to an overprediction of high rainfall classes. The choice of weights can be considered as part of hyperparameter tuning. A possible alternative way to address the class imbalance problem would be to adopt so-called effective number of samples \citet{Cui2019Class-BalancedSamples} that tries to to capture the diminishing marginal benefits by using more
data points of a class allowing for less compensation of the imbalance.


% ==== Section 3 ============================================================
\section{Comparison with Similar Studies}

The literature on precipitation nowcasting is fairly recent and therefore the available studies are somewhat limited in number and scope. Nonetheless, some studies that focus on a task similar to the one addressed in this thesis exist. In these studies the precipitation nowcasting problem is almost always addressed at a time resolution of 5min between each input frame. \citet{Agrawal2019MachineImages} and \citet{Ayzel2020RainNetNowcasting} propose a network architecture (U-Net) and study setup most similar to the one of this project. They both found better performance of their respective models for their tasks compared to persistence, this result remained for increasing lead times in the case of \citet{Ayzel2020RainNetNowcasting} where the output of the model was used as input for subsequent frames. The two studies compared the U-Net to the HRRR model and optical flow respectively with input frames 10min and 5min apart. Such a comparison (i.e. a comparision with a a model used in operational settings) could not be made for the neural network used in this thesis but it is reasonable to assume that with better hyperparameter tuning and small changes to the architecture a similar result could be achieved.

\citet{Shi2017DeepModel} propose a recurrent architecture based on long and short term memory (LSTM) called TrajGRU. The peculiarity of the model is to learn the connection structure between the hidden states. It utilizes the current input and the previous state to generate a local neighbourhood that should influence a certain location at a every timestamp. The model was compared to traditional Convolutional Neural Networks and to the predecessor of TrajGRU: ConvGRU. In an offline setting a comparison with persistence and the model was made. In all cases the absolute performance of the model decreases as the precipitation rate increases. TrajGRU was found to always outperform all other models as well as persistence.

In the study by \citet{Ravuri2021SkillfulRadar} the authors test a Generative Adversarial Network (GAN) architecture against three competitive models: a U-Net implementation, PySTEPS \citep{Pulkkinen2019Pysteps:v1.0} and an implementation of MetNet \citep{Snderby2020MetNet:Forecasting}. The GAN consists of two parts, a generator and a discriminator. The generator is responsible for creating images (given an input) that should be as similar as possible to the ones in the dataset. The discriminator has to learn to distinguish between a real example and a generated one. \citet{Ravuri2021SkillfulRadar} showed that the GAN is chosen by expert forecasters from the Met Office as the one with the highest usefulness compared to the alternative proposed models. 

Overall the studies suggest that the deep learning perspective is a promising approach to the precipitation nowcasting problem. In this regard this thesis confirms the identified trajectory and warrants further investigation into the application of neural networks to the 1h time scale. Possible extensions include the study of the behaviour of a U-Net with additional input frames, additional physical variables other than precipitation, additional output frames, the utilization of the model outputs as inputs for extending the lead time of the prediction (as in \citet{Ayzel2020RainNetNowcasting}). All these developments have as a precondition the betterment of the current model, specifically with better hyperparameter tuning and a different weighting strategy for the various precipitation classes.